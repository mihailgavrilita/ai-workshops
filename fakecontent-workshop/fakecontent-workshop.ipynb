{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6da0bd-0283-40a3-b03e-540f8b6e9cc3",
   "metadata": {},
   "source": [
    "### Fake Content Workshop -- AI in Fake Content Detection\n",
    "Welcome to our Fake Content Workshop!\n",
    "We hope that today you will learn some practical skills in applying AI tools in Fake Content Detection.\n",
    "Enjoy and don't hesitate to ask questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1ba79-a630-4cb0-a6b2-54d527d4ea9f",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before we start, let's make sure that we have everything installed on our machines:\n",
    "\n",
    "- Python;\n",
    "- Pyenv (for Linux, Mac or WLS);\n",
    "- Our libraries (dependancies);\n",
    "- Jupyter Lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171b4da-eefe-4b61-b78b-d5fe37f78dcb",
   "metadata": {},
   "source": [
    "### Sentimental Analysis\n",
    "\n",
    "Fake content tends to be written in a very positive (or very negative) tone.\n",
    "Finding the tone of a piece of text can be done with many AI models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b957e4-65a9-46dc-8587-42e9fabbbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def sentimental_analysis(text):\n",
    "    pipe = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\", truncation=True)\n",
    "    result = pipe(text)\n",
    "    \n",
    "    return result[0][\"label\"]\n",
    "\n",
    "sentimental_analysis(\"I love this product! It's amazing and works perfectly.\")\n",
    "# sentimental_analysis(\"El libro estuvo más o menos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e3723-b9f3-46f4-b72e-2d138dd7bb12",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "Some models are trained to work with only a specific subset of languages.\n",
    "If you would like to analyze media written in another language (but can't find a model that works with that language), translation could come in handy.\n",
    "Gladly, there are already tools that excell an this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a2a44-9ed1-48b4-9b33-052b6fed6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "api_key = \"YOUR_API_KEY_HERE\" # Put your api key here\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def translate(text, language):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an interpreter. Your tasks include translating text from one language to another. Answer without additional, just the translation.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Please translate the following text to {language}:\\n\\n{text}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_completion_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# translate(\"Я в восторге от этого нового гаджета!\", \"English\")\n",
    "# translate(\"Я в восторге от этого нового гаджета!\", \"Romanian\")\n",
    "# translate(\"Я в восторге от этого нового гаджета!\", \"Spanish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaffb6b-e069-401e-a3c6-7beca2443988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises:\n",
    "# 1. Connect to another translation tool (e.g. DeepL Translator, Google Translate)\n",
    "# 2. Compare the resulting tranlations. Which tools worked best for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d6f3f-422e-4d9f-906c-d706dfca5b6c",
   "metadata": {},
   "source": [
    "### Sumarization\n",
    "\n",
    "When analyzing longer texts, it is useful to first visualize a preview, to decide how important this article might be.\n",
    "Also, other tools might benefit from a shorter text, while keeping the main idea of it.\n",
    "Fortunately, pretrained models already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e0381-fda1-4224-b526-e9b375eec683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize(text):\n",
    "    summarizer = pipeline(task=\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", truncation=True)\n",
    "    result = summarizer(text)\n",
    "    \n",
    "    return result[0][\"summary_text\"]\n",
    "\n",
    "# summarize(\"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629603e-b35d-4c43-ae65-40b0e214502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises:\n",
    "# 1. Connect to another summarization tool (e.g. Groq API)\n",
    "# 2. Compare the resulting summarizations. Which tools worked best for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e6aa7-7569-4368-9300-b9dbf1c3b5fd",
   "metadata": {},
   "source": [
    "### All Together\n",
    "\n",
    "Now lets use the tools to analyze an example article!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ad893-6043-4619-b8e6-c3c46be4cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "Село Копанка вполне может стать понятием нарицательным, характерным для всей Молдовы.\n",
    "То есть, живут граждане Молдовы в селе, почти обычном. Из общего ряда Копанка выбивается тем, что хотя и находится на правом берегу Днестра и под юрисдикцией властей Кишинева, подключена к энергетическим системам Приднестровья. То есть, газ и электричество получают от тираспольских предприятий. И платят, соответственно, по тарифам в разы ниже, чем, жители остальной Молдовы. Им можно было только позавидовать.\n",
    "Как и все Приднестровье жители Копанки (и еще 11 сел зоны безопасности) - еще раз, это граждане Молдовы, - остались без газа и при веерных отключениях электричества.\n",
    "Жители села замечают, что “5 лет ими власти правобережной Молдовы не интересовались”, а тут явилась сама Санду с заявлениями о “цене свободы”. Причем, не сразу, как сотни людей попали в беду, а через полторы недели.\n",
    "Президент затянула старую песню о том, что Кремль виноват, что надо думать прежде всего о том, что ПАС подразумевает под свободой, а не о детях, которые замерзают в домах, что-то о биотопливе… - ее, естественно, послали… к европейской матери.\n",
    "Она не привезла им НИ ОДНОГО генератора, да даже одеяла не подарила или вязанку дров. Она пришла им прочесть мантру о евроинтеграции и “защите” от России….\n",
    "Что делает адекватный президент в такой ситуации? Везет генераторы в пострадавшие села, бесплатные дрова, разворачивает полевую кухню. Столько кредитов получали от Европы на энергетическую независимость, а когда “благодаря Украине” Молдова стала полностью независимой от газпромовской трубы (не от российского газа, его закупают в Европе по бешеным ценам), стали возмущаться попытками Кремля дестабилизировать ситуацию в Молдове. Куда уж больше, чем Санду и ПАС дестабилизировать…\n",
    "Кстати, а кредиты где?!\n",
    "Чтобы и денежки не потерять, и “против Кремля” выступить Санду вместо того, чтобы выполнить свое обещание (записано на видео), данное на пресс-конференции о том, что не будет препятствовать поставкам газа в Приднестровье, отправляет в Копанку частную компанию - поставщика электроэнергии, который собрался подключить село к электросистеме правобережной Молдовы. Газ Приднестровью нужен не только для частного потребителя и предприятий, но и для выработки электроэнергии, в том числе, и для Копанки. И когда Россия решила поставлять необходимый для Левобережья объем газа (через европейских посредников), власти Кишинева стали чинить препятствия.\n",
    "И тут в Копанку привозят электрические столбы и, не спросив мнение жителей села, начинают устанавливают линию. Тарифы при этом будут кишиневские, то есть в 4 раза выше.\n",
    "Люди, которые получают молдавские пенсии и приднестровские и молдавские зарплаты, возмутились. Мало того, никто им не предлагал ничего, никакую помощь, их ни о чем не спрашивали, им не предоставили право выбора, они должны были исполнить роль статистов в неудавшейся пиар-кампании президента.\n",
    "Они не хотят жить так, как живут люди в правобережной Молдове, откуда бегут толпами за границу навсегда. Они не хотят жить так, как предлагает им президент Санду и ее партия.\n",
    "Копанка показала, что думают о властях люди в Молдове. Об этой “цене свободы”, о праве на выбор, которого их лишили. Если обойти другие села Молдовы, можно услышать примерно тоже самое, только мнение это не будет услышано - провластная пресса упорно молчит и развивает, как в известные времена, культ личности Санду. А Копанка в эту благостную картинку не вписывается.\n",
    "Вот вам и выборы президента Молдовы, когда проголосовали против Санду.\n",
    "Еще одно доказательство, что жители страны не хотят эту “кризисную” власть.\n",
    "\"\"\"\n",
    "\n",
    "translated = translate(article, \"English\")\n",
    "print(translated)\n",
    "\n",
    "summarized = summarize(translated)\n",
    "print(summarized)\n",
    "\n",
    "sentiment = sentimental_analysis(article)\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68790f-8b52-4885-9df6-d6a8daf302ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises:\n",
    "# 1. What other metrics can be considered useful when deciding whether some content is fake?\n",
    "# 2. Find a model that could detect / measure such metric and use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357baa0-f427-4b49-9829-9a82beab6c01",
   "metadata": {},
   "source": [
    "### Fine-tuning Your Own Model\n",
    "\n",
    "If you can't find a model that would suit your needs (or you'd like to work with a particular dataset), you'll need to train your own model.\n",
    "The fastest way to do this is to take an already trained model and fine tune it with your own dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f5061-4890-4c5f-a4fc-f318586449e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: https://github.com/nastea19\n",
    "Source: https://github.com/nastea19/AI-in-actiune/blob/main/ml.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Define model save path\n",
    "model_dir = './saved_model'\n",
    "\n",
    "# Load datasets\n",
    "true_news = pd.read_csv('archive/True.csv')\n",
    "fake_news = pd.read_csv('archive/Fake.csv')\n",
    "\n",
    "# Add labels\n",
    "true_news['label'] = 1\n",
    "fake_news['label'] = 0\n",
    "\n",
    "# Combine datasets\n",
    "news_dataset = pd.concat([true_news, fake_news], ignore_index=True)\n",
    "\n",
    "# Preprocess the dataset\n",
    "news_dataset = news_dataset[['text', 'label']]\n",
    "news_dataset = news_dataset.sample(frac=0.05, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    news_dataset['text'].tolist(), news_dataset['label'].tolist(), test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052774a-2e29-44d7-962a-fd62dfb37bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Clean the training and validation texts\n",
    "train_texts = [clean_text(text) for text in train_texts]\n",
    "val_texts = [clean_text(text) for text in val_texts]\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae0a2a-1808-4167-9b97-97da8817e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset class\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "\n",
    "# Load the model and move to the device if already trained\n",
    "if os.path.exists(model_dir):\n",
    "    print(\"Modelul antrenat a fost gasit. Se incarca modelul salvat...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "else:\n",
    "    print(\"Modelul antrenat nu a fost gasit. Incepem antrenarea...\")\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,  # Increased epochs for better training\n",
    "        per_device_train_batch_size=4,  # Adjusted batch size for better performance\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Define evaluation metrics\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    # Create the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation_results = trainer.evaluate()\n",
    "    print(\"Evaluation Results:\", evaluation_results)\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a3dff-4f2a-42f6-a913-d132ca792a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_news(news_text):\n",
    "    model_for_prediction = model.to('cpu')\n",
    "    inputs = tokenizer(clean_text(news_text), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to('cpu') for key, val in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_for_prediction(**inputs)\n",
    "    \n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"True\" if prediction == 1 else \"False\"\n",
    "\n",
    "# Example usage\n",
    "news_example = input(\"Enter the news article text for verification: \")\n",
    "result = predict_news(news_example)\n",
    "print(f\"Prediction for the news article: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492a26e-afc6-440b-a4a1-23916dacce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises:\n",
    "# 1. How can you test the performance of your fine-tuned model?\n",
    "# 2. Think of other media that could be analyzed (e.g. Fake Audio, Fake Images, Fake Videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142b93f-f0f0-45ab-aebb-e15f607d1b03",
   "metadata": {},
   "source": [
    "### Fake Audio Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f16f52-f743-41ef-8ac0-2ee28d3b7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "# Load model and move to available device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"WpythonW/ast-fakeaudio-detector\"\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Process multiple audio files\n",
    "audio_files = [\"audio2.mp3\"]\n",
    "processed_batch = []\n",
    "\n",
    "for audio_path in audio_files:\n",
    "    # Load audio file\n",
    "    audio_data, sr = sf.read(audio_path)\n",
    "    \n",
    "    # Convert stereo to mono if needed\n",
    "    if len(audio_data.shape) > 1 and audio_data.shape[1] > 1:\n",
    "        audio_data = np.mean(audio_data, axis=1)\n",
    "    \n",
    "    # Resample to 16kHz if needed\n",
    "    if sr != 16000:\n",
    "        waveform = torch.from_numpy(audio_data).float()\n",
    "        if len(waveform.shape) == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        \n",
    "        resample = torchaudio.transforms.Resample(\n",
    "            orig_freq=sr, \n",
    "            new_freq=16000\n",
    "        )\n",
    "        waveform = resample(waveform)\n",
    "        audio_data = waveform.squeeze().numpy()\n",
    "    \n",
    "    processed_batch.append(audio_data)\n",
    "\n",
    "# Prepare batch input\n",
    "inputs = extractor(\n",
    "    processed_batch,\n",
    "    sampling_rate=16000,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# Process results\n",
    "for filename, probs in zip(audio_files, probabilities):\n",
    "    fake_prob = float(probs[0].cpu())\n",
    "    real_prob = float(probs[1].cpu())\n",
    "    prediction = \"FAKE\" if fake_prob > real_prob else \"REAL\"\n",
    "    \n",
    "    print(f\"\\nFile: {filename}\")\n",
    "    print(f\"Fake probability: {fake_prob:.2%}\")\n",
    "    print(f\"Real probability: {real_prob:.2%}\")\n",
    "    print(f\"Verdict: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240ce22-7f80-404f-a6d4-632530e13e79",
   "metadata": {},
   "source": [
    "### Fake Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58d891-4900-4ce6-abfa-1ebaec9c9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def deepfake_image_detection(image_url):\n",
    "    classifier = pipeline(task=\"image-classification\", model=\"Wvolf/ViT_Deepfake_Detection\")\n",
    "    preds = classifier(image_url)\n",
    "\n",
    "    return preds\n",
    "\n",
    "# deepfake_image_detection(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n",
    "# deepfake_image_detection(\"https://wallpapers.com/images/hd/woman-with-blue-eyes-face-reference-vwq2iertkzrzgkdu.jpg\")\n",
    "# deepfake_image_detection(\"https://static.fotor.com/app/features/img/aiface/realistic/1.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5c595-704c-4382-ab29-c8887ac51964",
   "metadata": {},
   "source": [
    "### References\n",
    "Congratulations for reaching here!\n",
    "For those interested, we've left a couple of references you might find useful in your own projects.\n",
    "Good luck!\n",
    "\n",
    "0. [AI Workshops Repo](https://github.com/mihailgavrilita/ai-workshops);\n",
    "1. [Fine-tuning Your Own Model Repo](https://github.com/nastea19/AI-in-actiune/tree/main) -- please give them a star if you found this repo useful;\n",
    "1. [Pyenv GitHub Repo](https://github.com/pyenv/pyenv);\n",
    "2. [Install Jupyter Lab](https://jupyter.org/install);\n",
    "3. [Hugging Face](https://huggingface.co/);\n",
    "4. [Multilingual Sentiment Analysis](https://huggingface.co/tabularisai/multilingual-sentiment-analysis);\n",
    "5. [Summarization](https://huggingface.co/docs/transformers/task_summary#summarization);\n",
    "6. [Clickbait Detection](https://huggingface.co/valurank/distilroberta-clickbait);\n",
    "7. [Fine-tune a Pretrained Model](https://huggingface.co/docs/transformers/training);\n",
    "8. [Groq Playground](https://console.groq.com/playground);\n",
    "9. [DeepL Translator API](https://www.deepl.com/en/pro-api);\n",
    "10. [Google Translate Python Client Library](https://cloud.google.com/translate/docs/reference/libraries/v2/python);\n",
    "11. [A Test Article](https://md.kp.media/daily/27653.5/5038090/);\n",
    "12. [Fake and Real News Dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
